%\begin{document}
	
\section{currentstuff}

ForwardMode: $J \cdot v$ : Jacobi * Vektor\\
BackwardMode: $v^T \cdot J$: Vektor$^T$ * Jacobi

Ziel: effiziente Auswertung von $v^TF'(x) = \dot{y}$
Überlegung: $\dot{y} = P \Phi'_l(V_l)\cdot \Phi'_{l-1}(V_{l-1})\dots \Phi'_1(V_1)\cdot Q \cdot \dot{x}$

wie im Forward Mode zu berechnen muss nun

$\dot{x} = \bar{y}^TP\Phi'_l(V_l)\dots \Phi'_1(V_1)\cdot Q$
ausgewertet werden

Non-Incremental:

\begin{tabular}{| L L |}
	v_{i-n} = x &i=1\dots n\\
	v_i=\varphi_i(v_j)_{j\prec i} &i=1 \dots l\\
	y_{m-i} = v_{l-i} & i=m-1 \dots 0\\
	\hline
	\bar{y}^TP\bar{v}_{l-i} = \bar{y}_{m-l}& i =m-1 \dots 0\\
	\bar{v}^T\Phi'(\dots)\bar{v}_j = \sum_{i\succ j} \bar{v}_j&\\
	&\\
\end{tabular}

Incremental: (Zusatzmaterial)

\begin{tabular}{| L L |}
	v_{i-n} = x &i=1\dots n\\
	v_i=\varphi_i(v_j)_{j\prec i} &i=1 \dots l\\
	y_{m-i} = v_{l-i} & i=m-1 \dots 0\\
	\hline
	\bar{v}_i = 0 & i =1-n \dots l\\
	\bar{v}_{l-1} = &\\
	\bar{v}^T\Phi'(\dots)\bar{v}_j = \sum_{i\succ j} \bar{v}_j&\\
	&\\
\end{tabular}


$\rightarrow$ Auswertung von $\bar{x} = \bar{y}^TF'(x)$ benötigt eine Auswertung von F (Forward-Sweep) und einen Reverse Sweep. Der Aufwand kann nach oben abgeschätzt werden

$$OPS(EVAO(\bar{y}^TF'(x))) \leq c_2 OPS(EVAL(F(k)))$$,
wobei $C_2$ eine Konstante ist $(c_2\approx 2-5)$
\\
Fehler bisher, sollte sein: (inkorrekte Korrektur, unvollständig)
$$\bar{v}_i \sum_{i\succ j} \bar{v}_i \frac{\partial}{\partial v_j}\varphi_i(u_i)$$


Bsp. Reverse Sweep (Non-Incremental) $f(x_1,x_2)$
wir wollen berechnen $\bar{y}^Tf'(x), f: \mathbb{R}^2\mapsto\mathbb{R}^1$

\begin{tabular}{| L  L |}
	\bar{y}_1 = 1 &\\
	\bar{v}_6 = \bar{y}_1 &\\
	\bar{v}_5 = \bar{v}_6 \frac{\partial}{\partial v_5}(v_4+v_5) & = \bar{v}\cdot 1\\
	\bar{v}_4 = \bar{v}_6 \frac{\partial}{\partial v_4}(v_4+v_5)& = \bar{v}\cdot 1\\
	\bar{v}_3 = \bar{v}_4 \frac{\partial}{\partial v_3}(\frac{v_2}{v_3})&  = \bar{v}_4 \cdot \left( -\frac{v_2}{v_3^2}\right)\\
	\bar{v}_2 = \bar{v}_4 \frac{\partial}{\partial v_2}(\frac{v_2}{v_3})&) =\bar{v}_4 \cdot \left(\frac{1}{v_3}\right)\\
	\bar{v}_1 = \bar{v}_2 \frac{\partial}{\partial v_1}(v_0 \cdot v_1) & =\bar{v}_2 \cdot v_0\\
	\bar{v}_0 = \bar{v}_2 \frac{\partial}{\partial v_0}(v_0 \cdot v_1) + \bar{v}_5 \frac{\partial}{\partial v_0}(cos(v_0))& =  =\bar{v}_2 \cdot v_1 + \bar{v}_5(-sin(v_0))\\
	\bar{v}_{-1} = \bar{v}_2 \frac{\partial}{\partial v_0}(v_0 \cdot v_1) + \bar{v}_5 \frac{\partial}{\partial v_0}(cos(v_0))& =2 \bar{v}_1 v_{-1} + \bar{v}_3 cos(v_{-1})\\
%	\hline
\end{tabular}

\vspace{\baselineskip}
Aufgaben:
\begin{itemize}
	\item Fehler von \ref{subsec:numerisch / Differenzenquotient} nachimplementieren
	\item Forward Mode implementieren
	\item Rosenbrock
\end{itemize}

Datentyp adouble erstellen, der 2 Werte speichert: einen für den Wert selbst und eines für die Richtung -> wichtiger Teil Fkt. a la operator+ 

in Matlab: Adimat (Uni Darmstadt)
\\...............................................


Wdh. Checkpointing
Gegeben $F = F_v \odot F_{v-1} \odot \dots \odot F_1$ D.H.
$x_0 = x \rightarrow x_1 = F_1(x_0) \rightarrow x_2 = F_2(x_1) \rightarrow \dots \rightarrow y= x_v = F_v(x_{v-1})$

Problem: bei Rückwärtssweep müssen alle Werte $x_i$ vorhanden sein (gespeichert werden)

$\bar{x}= \bar{y}^TF'(x) | \bar{x} = \bar{y}_1F'(x_0)  \leftarrow \dots \leftarrow $

Lösung: Checkpoints (CP) - anstatt aller $x_i$  werden nur ausgewählte Werte $x_i$ gespeichert, fehlende werden neuberechnet basierend auf diesen CP

Bsp:

\begin{tabular}{ L L L}
	\text{Startwert} & \text{CP-werte} & \text{Zielwert}\\
	0 \rightarrow & 1 \rightarrow 2 \rightarrow \dots \rightarrow 9 \rightarrow & 10 \\	
\end{tabular}

Vorgehen: Init 0


\begin{tabular} { c  L L L }
	Eval + Save & & & \\
	CP : & 0 \rightarrow & 1 \rightarrow 2 \rightarrow \dots \rightarrow 9 \rightarrow & 10 \\
	& \downarrow & \dots \downarrow \dots \downarrow \dots & \\
	Eval + Rev & 9 \rightleftarrows 10 & & \\
	&  \downarrow & & \\
	Load CP+ Eval & 7 \rightarrow 8 & &\\
	Eval + Rev & 8 \rightarrow 9 & &\\
	&  \downarrow & & \\
	Load CP+ Eval & 7 \rightleftarrows 8 & &\\
	
	letzte beiden Load CP + (Eval) + Rev & & &\\
\end{tabular}

Frage: wie werden die Checkpoints verteilt?
$\rightarrow$ Faustregel: kann als dynamisches Optimierungsproblem definiert werden \glqq$MIN t(L,C)$\grqq, wobei 
$$t(l,C) = MIN_{1\leq \tilde{l}\leq} \{\tilde{l} + t( l- \tilde{l},c-1)+t(\tilde{l},c)\}$$
mit
$ t(l,c) \approx$ Zeit für RV von $l$ Teilfkt mit C Checkpoints\\

\vspace{1cm}

Grafik\\

\noindent
Optimierungsproblem hat explizite Form \marginpar{nicht Prüfungsrelevant}
$$t(l,c) = v(l,c) \cdot l - \beta (c+1, r-1)$$
wobei $\beta(c,r) = \frac{(c+v)!}{c!r!}$ und $r = r(l,c)$
die eindeutige natürliche Zahl mit $\beta (c,r-1) < l \leq \beta (c,r)$

(r $\approx$ Ganzzahliges Verhältnis von Kosten für Reversal von l Funktionen mit c Checkpoints zu den Kosten für eine Vorwärtsauswertung)

Prop 12.3 \glqq Bin. Rev. Schedule \grqq\\
die optimalen CP sind gegeben durch 
$$ l(c,r) = \beta(c,r) = \binom{c+v}{c} = \frac{(c+r)!}{c!r!}$$

Faustregel:
\begin{itemize}
	
	\item CP ungefähr bei Hälfte des verbliebenen Abschnitts setzen

	Grafik
	\item wenn viele CP vorhanden (im Vgl. zu Rel. Auswertungsaufwand), dann CP eher link von der Mitte
	
	Grafik
	\item ansonsten eher rechts
	
	Grafik
\end{itemize}


..............................................

\section{Sparse Matrices / Dünnbesetzte Matritzen}
\label{"chap:sparsemats"}

Gegeben sei eine Funktion $F:\mathbb{R}^n\mapsto\mathbb{R}^m$ deren Jacobi-Matrix dünnbesetzt ist, d.h. nur wenige Elemente $\neq 0 $ besitzt


Ziel: effizientes Bestimmen dieser Einträge

Für den generellen Fall haben wir gesehen, dass der FM eine effiziente Methode liefert $F'(x)\cdot s$ auszuwerten.

Somit war es möglich $F'(x)$ durch eine geeignete Wahl von Richtungen  $s_1,\dots$ auszuwerten, z.B. mit den Einheitsvektoren $e_1,e_2, \dots e_n$, lieferte uns $F'(x)$ spaltenweise.\\



\noindent
Ziel: Bestimmung von $F'(x)$ mit $p <<n$ FM Auswertungen\\
Frage: Wie groß ist $p$ und wie sind die Richtungen $s_1,\dots s_p$ zu wählen.\\
Bem: Die Matrix $S = [s_1, \dots s_p] \in \mathbb{R}^{n\times p}$ wird oft als \glqq Seed\grqq Matrix bezeichnet.

MIt $S$ lässt sich die Auswertung des FM mit den $p$ Richtungen kur als 
$$B = [ F'(x)s_1 , \dots , F'(x)s_p] = F'(x) S = F'(x) [s_1,\dots,s_p]\ \ \ \ \  B= \dot{y},\ S=\dot{x}$$
darstellen, wobei die Zeilen $b_i^T$ von B durch $b_i^T = \nabla F_i(x)^TS$ gegeben sind

Beispiel mit Matrizen

M.A.W. wie ist $S\in\mathbb{R}^{n\times p}$ zu wählen, damit $B$ alle Informationen über $F'(x)$ enthält, so dass man diese zurückberechnen kann.

Beispiel matrixgrafik mit Sparsity pattern

oder genauer, wann lassen sich alle GLS $b_i^T = r_i^TS$ eindeutig nach $r_i$ lösen.
$\Rightarrow$ es werden mindestens genau so viele Gleichungen wie unbekannte benötigt\\
$\Rightarrow p \ge P_i$,\\

d.h. $P \geq P_i$, wobei $p_i$ die Anzahl der Nichtnulleinträge der $i$-ten Zeile von $F'(x)$ ist. Die Indexmenge dieser N.N.Einträge wird mit $\chi_i$ berzeichnet
$$b_i^T = e_i^T A \cdot S = \sum_{j=1}^n e_i^T A e_j e_j^T S =\sum_{j\in \chi_i} e_i^T A e_j (e_j^T S) = a_I^T s_i \ \ \ \ \ A \leftarrow F'(x)$$

Dabei kann $s_I$ so partitioniert werden, dass $S_i = P_i [...]...$

Quadratisches LGS
$$\hat{S}_i^Ta_i = \hat{b}_i = (e_k^Tb_i)_{k=1,\dots p_i}$$
mit $p_i$ Gleichungen und $p_i$ Unbekannten.

Herausforderung: Lösen von $m$ kleinen LGS zur Bestimmung von allen Zeilen von $F'(x)$
